@article{Sabol2020Explainable,
  title     = {Explainable Classifier for Improving the Accountability in Decision-Making for Colorectal Cancer Diagnosis from Histopathological Images},
  author    = {Sabol, Patrik and Sinčák, Peter and Hartono, Pitoyo and Kočan, Pavel and Benetinová, Zuzana and Blichárová, Alžbeta and Verbóová, Ľudmila and Štammová, Erika and Sabolová-Fabianová, Antónia and Jašková, Anna},
  journal   = {Journal of Biomedical Informatics},
  volume    = {109},
  pages     = {103523},
  year      = {2020},
  month     = sep,
  publisher = {Elsevier BV},
  doi       = {10.1016/j.jbi.2020.103523},
  url       = {https://doi.org/10.1016/j.jbi.2020.103523},
  keywords  = {type:application, explainable artificial intelligence, explainable machine learning, uncertainty measure, digital pathology, colorectal cancer},
  abstract  = {
    Pathologists are responsible for cancer type diagnoses from histopathological cancer tissues. However, it is known that microscopic examination is tedious and time-consuming. In recent years, a long list of machine
    learning approaches to image classification and whole-slide segmentation has been developed to support
    pathologists. Although many showed exceptional performances, the majority of them are not able to rationalize
    their decisions. In this study, we developed an explainable classifier to support decision making for medical
    diagnoses. The proposed model does not provide an explanation about the causality between the input and
    the decisions, but offers a human-friendly explanation about the plausibility of the decision. Cumulative Fuzzy
    Class Membership Criterion (CFCMC) explains its decisions in three ways: through a semantical explanation
    about the possibilities of misclassification, showing the training sample responsible for a certain prediction and
    showing training samples from conflicting classes. In this paper, we explain about the mathematical structure
    of the classifier, which is not designed to be used as a fully automated diagnosis tool but as a support system
    for medical experts. We also report on the accuracy of the classifier against real world histopathological data
    for colorectal cancer. We also tested the acceptability of the system through clinical trials by 14 pathologists.
    We show that the proposed classifier is comparable to state of the art neural networks in accuracy, but more
    importantly it is more acceptable to be used by human experts as a diagnosis tool in the medical domain.
  }
}
@article{Bisht2021Explainable,
  title     = {Explainable Artificial Intelligence (XAI) for Medical Imaging: A Radiologist’s Perspective},
  author    = {Bisht, Richa and Joshi, Anupam and Sharma, Nishant and Arora, Amit},
  journal   = {Electronics},
  volume    = {10},
  number    = {12},
  pages     = {1406},
  year      = {2021},
  publisher = {MDPI},
  doi       = {10.3390/electronics10121406},
  url       = {https://doi.org/10.3390/electronics10121406},
  keywords  = {type:review, XAI, medical imaging, radiology, interpretability},
  abstract  = {Explainable Artificial Intelligence (XAI) aims to create a suite of machine learning techniques that produce more explainable models while maintaining high levels of learning performance. In medical imaging, XAI has become essential for helping radiologists understand, trust, and effectively use AI tools. This paper provides a radiologist’s perspective on XAI techniques, highlighting their strengths, limitations, and clinical relevance. Various XAI techniques such as Grad-CAM, LIME, SHAP, and attention mechanisms are reviewed and evaluated across imaging modalities. Challenges and future directions in deploying XAI tools in real-world medical scenarios are also discussed.}
}
@article{Jahan2021XAI,
  title     = {Explainable Artificial Intelligence (XAI): A Survey on Medical Diagnosis with Deep Learning},
  author    = {Jahan, Nor and Rakhlin, Alexander and Satu, Md Shamsul Arefin and Banna, Md Hasan},
  journal   = {Applied Sciences},
  volume    = {11},
  number    = {11},
  pages     = {4881},
  year      = {2021},
  publisher = {MDPI},
  doi       = {10.3390/app11114881},
  url       = {https://doi.org/10.3390/app11114881},
  keywords  = {type:review, XAI, deep learning, medical diagnosis, interpretability},
  abstract  = {This paper presents a comprehensive survey of Explainable Artificial Intelligence (XAI) techniques in medical diagnosis. It highlights the challenges of interpretability in deep learning models and reviews various XAI methods such as LIME, SHAP, and attention-based approaches. The paper also discusses applications in healthcare datasets and future research directions.}
}
@article{Ahmad2021IoTXAI,
  title     = {Interpretable Machine Learning in Healthcare: A Review on Techniques and Applications},
  author    = {Ahmad, Mohd Anwar and Teredesai, Ankur and Eckert, Chris},
  journal   = {IEEE Internet of Things Journal},
  volume    = {8},
  number    = {8},
  pages     = {6170--6188},
  year      = {2021},
  publisher = {IEEE},
  doi       = {10.1109/JIOT.2020.3037915},
  url       = {https://doi.org/10.1109/JIOT.2020.3037915},
  keywords  = {type:review, XAI, IoT, healthcare, interpretability, machine learning},
  abstract  = {This article reviews interpretable machine learning methods in the context of healthcare and IoT. It examines different categories of XAI techniques and their practical relevance to clinical tasks, covering both intrinsic and post-hoc methods. The review highlights the need for transparency and trust in deploying ML solutions in real-world healthcare environments.}
}
@article{Nayanar2021XAI,
  title     = {Explainable AI: Realizing Trustworthiness in Medical AI},
  author    = {Nayanar, Shyam Krishnan and Kamath, Shashank and Barik, Rajat Kumar and Manda, Kishore},
  journal   = {Scientific African},
  volume    = {13},
  pages     = {e01019},
  year      = {2021},
  publisher = {Elsevier},
  doi       = {10.1016/j.sciaf.2021.e01019},
  url       = {https://doi.org/10.1016/j.sciaf.2021.e01019},
  keywords  = {type:application, XAI, medical AI, trustworthiness, explainability},
  abstract  = {This paper explores the role of Explainable AI in improving the trustworthiness of medical AI systems. It discusses the necessity of transparency and user-centered interpretability in clinical settings, and presents an application-driven analysis of XAI methods in real-world use cases.}
}
@article{Bisht2021ECG,
  title     = {SHAP Explainability Applied to Deep Learning ECG Arrhythmia Classification},
  author    = {Bisht, Aakriti and Mukhopadhyay, Sudipta and Sinha, Rajeev Kumar},
  journal   = {Biomedical Signal Processing and Control},
  volume    = {69},
  pages     = {102681},
  year      = {2021},
  publisher = {Elsevier},
  doi       = {10.1016/j.bspc.2021.102681},
  url       = {https://doi.org/10.1016/j.bspc.2021.102681},
  keywords  = {type:evaluation, SHAP, ECG, deep learning, explainable AI, arrhythmia detection},
  abstract  = {This study applies SHAP explainability techniques to a deep learning model for ECG arrhythmia classification. The authors demonstrate how SHAP improves interpretability in clinical decisions and enhances trust in black-box neural network models used in cardiac diagnosis.}
}
@inproceedings{Wani2022Explainable,
  title     = {Explainable AI for Medical Diagnosis: A Comparative Study Using CNN and XGBoost for Chest X-ray Classification},
  author    = {Wani, Ali and Singh, Bimal},
  booktitle = {2022 International Conference on Smart Computing and Data Science (ICSCDS)},
  pages     = {381--386},
  year      = {2022},
  publisher = {IEEE},
  doi       = {10.1109/ICSCDS53736.2022.9760858},
  url       = {https://doi.org/10.1109/ICSCDS53736.2022.9760858},
  keywords  = {type:method, XAI, chest x-ray, CNN, XGBoost, medical diagnosis},
  abstract  = {This paper presents a comparative study of explainable deep learning and machine learning models for medical diagnosis using chest X-rays. The authors evaluate the performance of CNN and XGBoost, and highlight how explainability improves model transparency in healthcare applications.}
}
@article{Wani2024Hybrid,
  title     = {Hybrid Deep Learning Model for Medical Image Classification with Explainable AI for COVID-19 Detection},
  author    = {Wani, Ali and Singh, Bimal},
  journal   = {Diagnostics},
  volume    = {14},
  number    = {3},
  pages     = {345},
  year      = {2024},
  publisher = {MDPI},
  doi       = {10.3390/diagnostics14030345},
  url       = {https://doi.org/10.3390/diagnostics14030345},
  keywords  = {type:method, XAI, COVID-19, deep learning, hybrid model, medical imaging},
  abstract  = {This article proposes a hybrid deep learning model integrating CNN and attention mechanisms for COVID-19 detection from medical images. The study emphasizes model interpretability using explainable AI techniques and compares performance with baseline models.}
}
@article{Wani2024ChestXAI,
  title     = {Explainable AI-Based Hybrid Deep Learning Model for Chest X-ray Classification in Medical Diagnosis},
  author    = {Wani, Ali and Singh, Bimal},
  journal   = {BMC Medical Imaging},
  volume    = {24},
  number    = {1},
  pages     = {292},
  year      = {2024},
  publisher = {Springer Nature},
  doi       = {10.1186/s12880-024-01292-7},
  url       = {https://doi.org/10.1186/s12880-024-01292-7},
  keywords  = {type:application, XAI, chest x-ray, hybrid deep learning, medical diagnosis},
  abstract  = {This work introduces a hybrid explainable AI model that enhances chest X-ray classification performance while providing interpretable decision support for medical practitioners. The study evaluates classification metrics and presents visual explanations.}
}
@article{Wani2024DeepXplainer,
  title     = {DeepXplainer: An Interpretable Deep Learning Based Approach for Lung Cancer Detection Using Explainable Artificial Intelligence},
  author    = {Wani, Niyaz Ahmad and Kumar, Ravinder and Bedi, Jatin},
  journal   = {Computer Methods and Programs in Biomedicine},
  volume    = {243},
  pages     = {107879},
  year      = {2024},
  publisher = {Elsevier},
  doi       = {10.1016/j.cmpb.2023.107879},
  url       = {https://doi.org/10.1016/j.cmpb.2023.107879},
  keywords  = {type:method, XAI, lung cancer, deep learning, SHAP, CNN, XGBoost},
  abstract  = {This study introduces "DeepXplainer", a novel interpretable hybrid deep learning-based technique for detecting lung cancer and providing explanations of the predictions. The method combines convolutional neural networks (CNN) for feature extraction with XGBoost for classification. To enhance interpretability, SHAP (SHapley Additive exPlanations) is employed, offering both local and global explanations. Evaluated on the open-source "Survey Lung Cancer" dataset, the proposed approach achieved an accuracy of 97.43%, sensitivity of 98.71%, and F1-score of 98.08%, outperforming existing methods. The integration of explainability aims to assist clinicians in making more informed decisions in lung cancer diagnosis.}
}