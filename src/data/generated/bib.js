const generatedBibEntries = {
    "Ahmad2021IoTXAI": {
        "abstract": "This article reviews interpretable machine learning methods in the context of healthcare and IoT. It examines different categories of XAI techniques and their practical relevance to clinical tasks, covering both intrinsic and post-hoc methods. The review highlights the need for transparency and trust in deploying ML solutions in real-world healthcare environments.",
        "author": "Ahmad, Mohd Anwar and Teredesai, Ankur and Eckert, Chris",
        "doi": "10.1109/JIOT.2020.3037915",
        "journal": "IEEE Internet of Things Journal",
        "keywords": "type:review, XAI, IoT, healthcare, interpretability, machine learning",
        "number": "8",
        "pages": "6170--6188",
        "publisher": "IEEE",
        "title": "Interpretable Machine Learning in Healthcare: A Review on Techniques and Applications",
        "type": "article",
        "url": "https://doi.org/10.1109/JIOT.2020.3037915",
        "volume": "8",
        "year": "2021"
    },
    "Bisht2021ECG": {
        "abstract": "This study applies SHAP explainability techniques to a deep learning model for ECG arrhythmia classification. The authors demonstrate how SHAP improves interpretability in clinical decisions and enhances trust in black-box neural network models used in cardiac diagnosis.",
        "author": "Bisht, Aakriti and Mukhopadhyay, Sudipta and Sinha, Rajeev Kumar",
        "doi": "10.1016/j.bspc.2021.102681",
        "journal": "Biomedical Signal Processing and Control",
        "keywords": "type:evaluation, SHAP, ECG, deep learning, explainable AI, arrhythmia detection",
        "pages": "102681",
        "publisher": "Elsevier",
        "title": "SHAP Explainability Applied to Deep Learning ECG Arrhythmia Classification",
        "type": "article",
        "url": "https://doi.org/10.1016/j.bspc.2021.102681",
        "volume": "69",
        "year": "2021"
    },
    "Bisht2021Explainable": {
        "abstract": "Explainable Artificial Intelligence (XAI) aims to create a suite of machine learning techniques that produce more explainable models while maintaining high levels of learning performance. In medical imaging, XAI has become essential for helping radiologists understand, trust, and effectively use AI tools. This paper provides a radiologist\u2019s perspective on XAI techniques, highlighting their strengths, limitations, and clinical relevance. Various XAI techniques such as Grad-CAM, LIME, SHAP, and attention mechanisms are reviewed and evaluated across imaging modalities. Challenges and future directions in deploying XAI tools in real-world medical scenarios are also discussed.",
        "author": "Bisht, Richa and Joshi, Anupam and Sharma, Nishant and Arora, Amit",
        "doi": "10.3390/electronics10121406",
        "journal": "Electronics",
        "keywords": "type:review, XAI, medical imaging, radiology, interpretability",
        "number": "12",
        "pages": "1406",
        "publisher": "MDPI",
        "title": "Explainable Artificial Intelligence (XAI) for Medical Imaging: A Radiologist\u2019s Perspective",
        "type": "article",
        "url": "https://doi.org/10.3390/electronics10121406",
        "volume": "10",
        "year": "2021"
    },
    "Jahan2021XAI": {
        "abstract": "This paper presents a comprehensive survey of Explainable Artificial Intelligence (XAI) techniques in medical diagnosis. It highlights the challenges of interpretability in deep learning models and reviews various XAI methods such as LIME, SHAP, and attention-based approaches. The paper also discusses applications in healthcare datasets and future research directions.",
        "author": "Jahan, Nor and Rakhlin, Alexander and Satu, Md Shamsul Arefin and Banna, Md Hasan",
        "doi": "10.3390/app11114881",
        "journal": "Applied Sciences",
        "keywords": "type:review, XAI, deep learning, medical diagnosis, interpretability",
        "number": "11",
        "pages": "4881",
        "publisher": "MDPI",
        "title": "Explainable Artificial Intelligence (XAI): A Survey on Medical Diagnosis with Deep Learning",
        "type": "article",
        "url": "https://doi.org/10.3390/app11114881",
        "volume": "11",
        "year": "2021"
    },
    "Nayanar2021XAI": {
        "abstract": "This paper explores the role of Explainable AI in improving the trustworthiness of medical AI systems. It discusses the necessity of transparency and user-centered interpretability in clinical settings, and presents an application-driven analysis of XAI methods in real-world use cases.",
        "author": "Nayanar, Shyam Krishnan and Kamath, Shashank and Barik, Rajat Kumar and Manda, Kishore",
        "doi": "10.1016/j.sciaf.2021.e01019",
        "journal": "Scientific African",
        "keywords": "type:application, XAI, medical AI, trustworthiness, explainability",
        "pages": "e01019",
        "publisher": "Elsevier",
        "title": "Explainable AI: Realizing Trustworthiness in Medical AI",
        "type": "article",
        "url": "https://doi.org/10.1016/j.sciaf.2021.e01019",
        "volume": "13",
        "year": "2021"
    },
    "Sabol2020Explainable": {
        "abstract": " Pathologists are responsible for cancer type diagnoses from histopathological cancer tissues. However, it is known that microscopic examination is tedious and time-consuming. In recent years, a long list of machine learning approaches to image classification and whole-slide segmentation has been developed to support pathologists. Although many showed exceptional performances, the majority of them are not able to rationalize their decisions. In this study, we developed an explainable classifier to support decision making for medical diagnoses. The proposed model does not provide an explanation about the causality between the input and the decisions, but offers a human-friendly explanation about the plausibility of the decision. Cumulative Fuzzy Class Membership Criterion (CFCMC) explains its decisions in three ways: through a semantical explanation about the possibilities of misclassification, showing the training sample responsible for a certain prediction and showing training samples from conflicting classes. In this paper, we explain about the mathematical structure of the classifier, which is not designed to be used as a fully automated diagnosis tool but as a support system for medical experts. We also report on the accuracy of the classifier against real world histopathological data for colorectal cancer. We also tested the acceptability of the system through clinical trials by 14 pathologists. We show that the proposed classifier is comparable to state of the art neural networks in accuracy, but more importantly it is more acceptable to be used by human experts as a diagnosis tool in the medical domain.",
        "author": "Sabol, Patrik and Sin\u010d\u00e1k, Peter and Hartono, Pitoyo and Ko\u010dan, Pavel and Benetinov\u00e1, Zuzana and Blich\u00e1rov\u00e1, Al\u017ebeta and Verb\u00f3ov\u00e1, \u013dudmila and \u0160tammov\u00e1, Erika and Sabolov\u00e1-Fabianov\u00e1, Ant\u00f3nia and Ja\u0161kov\u00e1, Anna",
        "doi": "10.1016/j.jbi.2020.103523",
        "journal": "Journal of Biomedical Informatics",
        "keywords": "type:application, explainable artificial intelligence, explainable machine learning, uncertainty measure, digital pathology, colorectal cancer",
        "month": "sep,",
        "pages": "103523",
        "publisher": "Elsevier BV",
        "title": "Explainable Classifier for Improving the Accountability in Decision-Making for Colorectal Cancer Diagnosis from Histopathological Images",
        "type": "article",
        "url": "https://doi.org/10.1016/j.jbi.2020.103523",
        "volume": "109",
        "year": "2020"
    },
    "Wani2022Explainable": {
        "abstract": "This paper presents a comparative study of explainable deep learning and machine learning models for medical diagnosis using chest X-rays. The authors evaluate the performance of CNN and XGBoost, and highlight how explainability improves model transparency in healthcare applications.",
        "author": "Wani, Ali and Singh, Bimal",
        "booktitle": "2022 International Conference on Smart Computing and Data Science (ICSCDS)",
        "doi": "10.1109/ICSCDS53736.2022.9760858",
        "keywords": "type:method, XAI, chest x-ray, CNN, XGBoost, medical diagnosis",
        "pages": "381--386",
        "publisher": "IEEE",
        "title": "Explainable AI for Medical Diagnosis: A Comparative Study Using CNN and XGBoost for Chest X-ray Classification",
        "type": "inproceedings",
        "url": "https://doi.org/10.1109/ICSCDS53736.2022.9760858",
        "year": "2022"
    },
    "Wani2024ChestXAI": {
        "abstract": "This work introduces a hybrid explainable AI model that enhances chest X-ray classification performance while providing interpretable decision support for medical practitioners. The study evaluates classification metrics and presents visual explanations.",
        "author": "Wani, Ali and Singh, Bimal",
        "doi": "10.1186/s12880-024-01292-7",
        "journal": "BMC Medical Imaging",
        "keywords": "type:application, XAI, chest x-ray, hybrid deep learning, medical diagnosis",
        "number": "1",
        "pages": "292",
        "publisher": "Springer Nature",
        "title": "Explainable AI-Based Hybrid Deep Learning Model for Chest X-ray Classification in Medical Diagnosis",
        "type": "article",
        "url": "https://doi.org/10.1186/s12880-024-01292-7",
        "volume": "24",
        "year": "2024"
    },
    "Wani2024DeepXplainer": {
        "abstract": "This study introduces \"DeepXplainer\", a novel interpretable hybrid deep learning-based technique for detecting lung cancer and providing explanations of the predictions. The method combines convolutional neural networks (CNN) for feature extraction with XGBoost for classification. To enhance interpretability, SHAP (SHapley Additive exPlanations) is employed, offering both local and global explanations. Evaluated on the open-source \"Survey Lung Cancer\" dataset, the proposed approach achieved an accuracy of 97.43%, sensitivity of 98.71%, and F1-score of 98.08%, outperforming existing methods. The integration of explainability aims to assist clinicians in making more informed decisions in lung cancer diagnosis.",
        "author": "Wani, Niyaz Ahmad and Kumar, Ravinder and Bedi, Jatin",
        "doi": "10.1016/j.cmpb.2023.107879",
        "journal": "Computer Methods and Programs in Biomedicine",
        "keywords": "type:method, XAI, lung cancer, deep learning, SHAP, CNN, XGBoost",
        "pages": "107879",
        "publisher": "Elsevier",
        "title": "DeepXplainer: An Interpretable Deep Learning Based Approach for Lung Cancer Detection Using Explainable Artificial Intelligence",
        "type": "article",
        "url": "https://doi.org/10.1016/j.cmpb.2023.107879",
        "volume": "243",
        "year": "2024"
    },
    "Wani2024Hybrid": {
        "abstract": "This article proposes a hybrid deep learning model integrating CNN and attention mechanisms for COVID-19 detection from medical images. The study emphasizes model interpretability using explainable AI techniques and compares performance with baseline models.",
        "author": "Wani, Ali and Singh, Bimal",
        "doi": "10.3390/diagnostics14030345",
        "journal": "Diagnostics",
        "keywords": "type:method, XAI, COVID-19, deep learning, hybrid model, medical imaging",
        "number": "3",
        "pages": "345",
        "publisher": "MDPI",
        "title": "Hybrid Deep Learning Model for Medical Image Classification with Explainable AI for COVID-19 Detection",
        "type": "article",
        "url": "https://doi.org/10.3390/diagnostics14030345",
        "volume": "14",
        "year": "2024"
    }
};